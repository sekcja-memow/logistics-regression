{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prime numbers recognition\n",
    "\n",
    "In this file we train a binary classifier on a subset of the MNIST set, in which we distinguish classes (digits 0 and 1 are to be excluded from the set):\n",
    "\n",
    "* Prime numbers (2,3,5,7)\n",
    "* Complex numbers (4,6,8,9)\n",
    "\n",
    "### Model testing\n",
    "\n",
    "We test our model with different parameters. Parameters such as number of iterations, threshold or number of samples differ and we can observe how it affects the whole model and its outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading packages and preparing notebook for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.19.4 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from -r ../requirements.txt (line 1)) (1.19.4)\n",
      "Collecting idx2numpy>=1.2.3 (from -r ../requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/6b/80628f6cc2f44d80b27f1ef7b57b257ed4c73766113b77d13ad110c091b4/idx2numpy-1.2.3.tar.gz\n",
      "Collecting requests>=2.25.0 (from -r ../requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl\n",
      "Collecting sklearn>=0.0 (from -r ../requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting matplotlib>=3.3.2 (from -r ../requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/0d/18c590b08f91d14376174bcf1acf96a9d4b35045f7344d046b60444399f2/matplotlib-3.3.3-cp37-cp37m-macosx_10_9_x86_64.whl (8.5MB)\n",
      "\u001b[K     |████████████████████████████████| 8.5MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from idx2numpy>=1.2.3->-r ../requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.25.0->-r ../requirements.txt (line 3)) (2019.9.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.25.0->-r ../requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.25.0->-r ../requirements.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.25.0->-r ../requirements.txt (line 3)) (1.24.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/kamil/.local/lib/python3.7/site-packages (from sklearn>=0.0->-r ../requirements.txt (line 4)) (0.23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.3.2->-r ../requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.3.2->-r ../requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.3.2->-r ../requirements.txt (line 5)) (2.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.3.2->-r ../requirements.txt (line 5)) (6.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.3.2->-r ../requirements.txt (line 5)) (2.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn>=0.0->-r ../requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn>=0.0->-r ../requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn>=0.0->-r ../requirements.txt (line 4)) (1.5.4)\n",
      "Requirement already satisfied: setuptools in /Users/kamil/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.3.2->-r ../requirements.txt (line 5)) (41.4.0)\n",
      "Building wheels for collected packages: idx2numpy, sklearn\n",
      "  Building wheel for idx2numpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idx2numpy: filename=idx2numpy-1.2.3-cp37-none-any.whl size=7905 sha256=664ceee2f99a8d1d96bda7cc47a2a255799e72410380ece41474c13d9ed2acb1\n",
      "  Stored in directory: /Users/kamil/Library/Caches/pip/wheels/7a/c1/da/284ce80a748fab898b8d1fa95468a386e7cf3b81da18511f9d\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=9cc552d7fc476ce0e35cb5bd2a2d6751ef9e35b96784c11b3f62b9c90e8df1bb\n",
      "  Stored in directory: /Users/kamil/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built idx2numpy sklearn\n",
      "Installing collected packages: idx2numpy, requests, sklearn, matplotlib\n",
      "  Found existing installation: idx2numpy 1.2.2\n",
      "    Uninstalling idx2numpy-1.2.2:\n",
      "      Successfully uninstalled idx2numpy-1.2.2\n",
      "  Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Found existing installation: matplotlib 3.1.1\n",
      "    Uninstalling matplotlib-3.1.1:\n",
      "      Successfully uninstalled matplotlib-3.1.1\n",
      "Successfully installed idx2numpy-1.2.3 matplotlib-3.3.3 requests-2.25.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# required packages installation\n",
    "\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing model and required components\n",
    "from core.models import LogisticRegression\n",
    "from core.optimizers import *\n",
    "from core.regularizers import *\n",
    "\n",
    "# imports for data loading\n",
    "from requests import get\n",
    "import idx2numpy\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# import for data processing and analysis\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# warnings ignoring\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "#### This section is responsible for downloading the data on which our model will be trained in many scenarios - with variety of different:\n",
    "- number of samples\n",
    "- number of iterations\n",
    "- thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data downloading and preprocessing function\n",
    "        \n",
    "def load_data():\n",
    "    \n",
    "    urls = []\n",
    "    urls.append('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')\n",
    "    urls.append('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')\n",
    "    urls.append('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz')\n",
    "    urls.append('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz')\n",
    "    \n",
    "    print('Downloading data...')\n",
    "    responses = [get(url, allow_redirects=True) for url in urls]\n",
    "    \n",
    "    names = ['train_images', 'train_labels', 'test_images', 'test_labels']\n",
    "    \n",
    "    for index, response in enumerate(responses):\n",
    "        open(f'./datasets/prime_numbers/{names[index]}-gz', 'wb').write(response.content)\n",
    "        \n",
    "    print('Unpacking files...')\n",
    "    for name in names:\n",
    "        with gzip.open(f'./datasets/prime_numbers/{name}-gz', 'rb') as f_in:\n",
    "            with open(f'./datasets/prime_numbers/{name}', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    " \n",
    "    for name in names:\n",
    "        os.remove(f'./datasets/prime_numbers/{name}-gz')\n",
    "\n",
    "    f_read = open('./datasets/prime_numbers/train_images','rb')\n",
    "    X_train = idx2numpy.convert_from_file(f_read)\n",
    "\n",
    "    f_read = open('./datasets/prime_numbers/train_labels', 'rb')\n",
    "    y_train = idx2numpy.convert_from_file(f_read)\n",
    "\n",
    "    f_read = open('./datasets/prime_numbers/test_images', 'rb')\n",
    "    X_test = idx2numpy.convert_from_file(f_read)\n",
    "    \n",
    "    f_read = open('./datasets/prime_numbers/test_labels', 'rb')\n",
    "    y_test = idx2numpy.convert_from_file(f_read)\n",
    "\n",
    "\n",
    "    X_train = X_train.reshape(len(X_train), 784)\n",
    "    X_test = X_test.reshape(len(X_test), 784)\n",
    "\n",
    "\n",
    "    tmp_X = []\n",
    "    tmp_y = []\n",
    "\n",
    "    for index,label in enumerate(y_train):\n",
    "        if label in [2,3,5,7]:\n",
    "            tmp_X.append(X_train[index])\n",
    "            tmp_y.append(1)\n",
    "        elif label in [4,6,8,9]:\n",
    "            tmp_X.append(X_train[index])\n",
    "            tmp_y.append(0)\n",
    "\n",
    "    tmp_X_test = []\n",
    "    tmp_y_test = []\n",
    "\n",
    "    for index,label in enumerate(y_test):\n",
    "        if label in [2,3,5,7]:\n",
    "            tmp_X_test.append(X_test[index])\n",
    "            tmp_y_test.append(1)\n",
    "        elif label in [4,6,8,9]:\n",
    "            tmp_X_test.append(X_test[index])\n",
    "            tmp_y_test.append(0)\n",
    "\n",
    "\n",
    "    X_train = np.array(tmp_X)\n",
    "    X_test = np.array(tmp_X_test)\n",
    "    y_train = np.array(tmp_y)\n",
    "    y_test = np.array(tmp_y_test)\n",
    "    \n",
    "    print('Data loading finished!')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n",
      "Unpacking files...\n",
      "Data loading finished!\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "\n",
    "X_train_raw, X_test, y_train_raw, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of training and testing samples + Single Record Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of our training samples: \t 47335\n",
      "Num of our testing samples: \t 7885\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Num of our training samples: \\t\", X_train_raw.shape[0])\n",
    "print(\"Num of our testing samples: \\t\", X_test.shape[0])\n",
    "\n",
    "number = np.reshape(X_train_raw[0], (28,28))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(number, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training sample\n",
    "\n",
    "##### Examples with labels attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "for i, (X, label) in enumerate(zip(X_train_raw[0:8], y_train_raw[0:8])):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.title(f'Label: {label}', fontsize = 20)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.reshape(X, (28,28)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking num of samples we want to train our model on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = 10000\n",
    "\n",
    "X_train = X_train_raw[:num_of_samples]\n",
    "y_train = y_train_raw[:num_of_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default model efficienty\n",
    "\n",
    "#### Cross validation score determination on model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "result = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result\n",
      "[0.9125 0.881  0.912  0.8985 0.82  ]\n",
      "Cross validation mean\n",
      "0.8847999999999999\n"
     ]
    }
   ],
   "source": [
    "print('Cross validation result')\n",
    "print(result)\n",
    "print('Cross validation mean')\n",
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Classification report is used to measure the quality of predictions from a classification algorithm. \n",
    "##### Main scores: \n",
    "##### **Precision** – What percent of your predictions were correct? (Precision = TP/(TP + FP))\n",
    "##### **Recall** – What percent of the positive cases did you catch? (Recall = TP/(TP+FN))\n",
    "##### **F1 score** – What percent of positive predictions were correct? (F1 Score = 2*(Recall * Precision) / (Recall + Precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending all optimizers to check their efficiency\n",
    "\n",
    "optimizers = [\n",
    "    GradientDescentOptimizer(),\n",
    "    MomentumGradientDescentOptimizer(),\n",
    "    AdaGradOptimizer(),\n",
    "    RMSPropOptimizer()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                 GradientDescentOptimizer(learning_rate: 0.03)                  \n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      3923\n",
      "           1       0.91      0.91      0.91      3962\n",
      "\n",
      "    accuracy                           0.91      7885\n",
      "   macro avg       0.91      0.91      0.91      7885\n",
      "weighted avg       0.91      0.91      0.91      7885\n",
      "\n",
      "================================================================================\n",
      "   MomentumGradientDescentOptimizer(learning_rate: 0.03, momentum_rate: 0.9)    \n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      3923\n",
      "           1       0.93      0.89      0.91      3962\n",
      "\n",
      "    accuracy                           0.91      7885\n",
      "   macro avg       0.91      0.91      0.91      7885\n",
      "weighted avg       0.91      0.91      0.91      7885\n",
      "\n",
      "================================================================================\n",
      "             AdaGradOptimizer(learning_rate: 0.03, epsilon: 1e-07)              \n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91      3923\n",
      "           1       0.92      0.91      0.91      3962\n",
      "\n",
      "    accuracy                           0.91      7885\n",
      "   macro avg       0.91      0.91      0.91      7885\n",
      "weighted avg       0.91      0.91      0.91      7885\n",
      "\n",
      "================================================================================\n",
      "        RMSPropOptimizer(learning_rate: 0.03, beta: 0.9, epsilon: 1e-07)        \n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      3923\n",
      "           1       0.92      0.92      0.92      3962\n",
      "\n",
      "    accuracy                           0.92      7885\n",
      "   macro avg       0.92      0.92      0.92      7885\n",
      "weighted avg       0.92      0.92      0.92      7885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regularizer=RidgeRegularizer(), num_iterations=300, threshold=0.5, fit_intercept=True, verbose=False\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    model = LogisticRegression(optimizer = optimizer)\n",
    "    model.fit(X_train_raw, y_train_raw)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"=\"*80)\n",
    "    print(str(optimizer).center(80, ' '))\n",
    "    print(\"=\"*80)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers testing\n",
    "\n",
    "##### Regularizers allow you to apply penalties on layer parameters or layer activity during optimization. These penalties are summed into the loss function that the network optimizes. \n",
    "\n",
    "#### Optimizers implemented in our code:\n",
    "- `Lasso` (least absolute shrinkage and selection operator) - it is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.\n",
    "- `Ridge` - in general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias.\n",
    "- `ElasticNet` - it is a regularized regression method that linearly combines the L1 and L2 penalties of the above *Lasso* and *Ridge* methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending all regularizers to check their efficiency\n",
    "\n",
    "regularizers = [\n",
    "    LassoRegularizer(),\n",
    "    RidgeRegularizer(),\n",
    "    ElasticNetRegularizer()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we test all three optimizers\n",
    "\n",
    "Default values provided to *LinearRegression* model are:\n",
    "- *num_iterations* = 300 <- number of iterations of the optimizer algorithm\n",
    "- *threshold* = 0.5 <- decision threshold\n",
    "- *fit_intercept* = True <- whether to calculate the intercept for this model\n",
    "- *verbose* = False <- output-related parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "        RMSPropOptimizer(learning_rate: 0.03, beta: 0.9, epsilon: 1e-07)        \n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      3923\n",
      "           1       0.90      0.94      0.92      3962\n",
      "\n",
      "    accuracy                           0.92      7885\n",
      "   macro avg       0.92      0.91      0.91      7885\n",
      "weighted avg       0.92      0.92      0.91      7885\n",
      "\n",
      "================================================================================\n",
      "        RMSPropOptimizer(learning_rate: 0.03, beta: 0.9, epsilon: 1e-07)        \n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89      3923\n",
      "           1       0.97      0.78      0.87      3962\n",
      "\n",
      "    accuracy                           0.88      7885\n",
      "   macro avg       0.89      0.88      0.88      7885\n",
      "weighted avg       0.89      0.88      0.88      7885\n",
      "\n",
      "================================================================================\n",
      "        RMSPropOptimizer(learning_rate: 0.03, beta: 0.9, epsilon: 1e-07)        \n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      3923\n",
      "           1       0.94      0.89      0.92      3962\n",
      "\n",
      "    accuracy                           0.92      7885\n",
      "   macro avg       0.92      0.92      0.92      7885\n",
      "weighted avg       0.92      0.92      0.92      7885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = RMSPropOptimizer, num_iterations=300, threshold=0.5, fit_intercept=True, verbose=False\n",
    "\n",
    "for regularizer in regularizers:\n",
    "    model = LogisticRegression(regularizer = regularizer)\n",
    "    model.fit(X_train_raw, y_train_raw)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"=\"*80)\n",
    "    print(str(optimizer).center(80, ' '))\n",
    "    print(\"=\"*80)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different amounts of samples during model training\n",
    "\n",
    "#### In this section we check how different amount of samples affects the whole model and how it affects scores, precision etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all samples:  47335\n"
     ]
    }
   ],
   "source": [
    "print('Number of all samples: ', len(X_train_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = [5, 100, 500, 1000, 2000, 5000, 10000, 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating models with different sample number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\" 5 samples \"====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.58      0.63      3923\n",
      "           1       0.64      0.75      0.70      3962\n",
      "\n",
      "    accuracy                           0.67      7885\n",
      "   macro avg       0.67      0.67      0.66      7885\n",
      "weighted avg       0.67      0.67      0.66      7885\n",
      "\n",
      "===================\" 100 samples \"===================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.82      3923\n",
      "           1       0.87      0.72      0.79      3962\n",
      "\n",
      "    accuracy                           0.81      7885\n",
      "   macro avg       0.82      0.81      0.81      7885\n",
      "weighted avg       0.82      0.81      0.81      7885\n",
      "\n",
      "===================\" 500 samples \"===================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87      3923\n",
      "           1       0.86      0.90      0.88      3962\n",
      "\n",
      "    accuracy                           0.88      7885\n",
      "   macro avg       0.88      0.88      0.88      7885\n",
      "weighted avg       0.88      0.88      0.88      7885\n",
      "\n",
      "===================\" 1000 samples \"==================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86      3923\n",
      "           1       0.83      0.93      0.88      3962\n",
      "\n",
      "    accuracy                           0.87      7885\n",
      "   macro avg       0.88      0.87      0.87      7885\n",
      "weighted avg       0.88      0.87      0.87      7885\n",
      "\n",
      "===================\" 2000 samples \"==================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      3923\n",
      "           1       0.91      0.88      0.90      3962\n",
      "\n",
      "    accuracy                           0.90      7885\n",
      "   macro avg       0.90      0.90      0.90      7885\n",
      "weighted avg       0.90      0.90      0.90      7885\n",
      "\n",
      "===================\" 5000 samples \"==================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91      3923\n",
      "           1       0.92      0.90      0.91      3962\n",
      "\n",
      "    accuracy                           0.91      7885\n",
      "   macro avg       0.91      0.91      0.91      7885\n",
      "weighted avg       0.91      0.91      0.91      7885\n",
      "\n",
      "==================\" 10000 samples \"==================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91      3923\n",
      "           1       0.92      0.91      0.91      3962\n",
      "\n",
      "    accuracy                           0.91      7885\n",
      "   macro avg       0.91      0.91      0.91      7885\n",
      "weighted avg       0.91      0.91      0.91      7885\n",
      "\n",
      "==================\" 20000 samples \"==================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      3923\n",
      "           1       0.91      0.92      0.92      3962\n",
      "\n",
      "    accuracy                           0.92      7885\n",
      "   macro avg       0.92      0.92      0.92      7885\n",
      "weighted avg       0.92      0.92      0.92      7885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num in num_of_samples:\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_raw[:num], y_train_raw[:num])\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(('\" '+ str(num) +' samples \"').center(53, '='))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different amounts of iterations during model training\n",
    "\n",
    "#### In this section we check how different amount of iterations affects the whole model and how it affects scores, precision etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_iterations = [100, 300, 500, 1000, 3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating models with different iteration number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\" 100 iterations \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.58      0.73      3923\n",
      "           1       0.70      0.98      0.82      3962\n",
      "\n",
      "    accuracy                           0.78      7885\n",
      "   macro avg       0.84      0.78      0.78      7885\n",
      "weighted avg       0.84      0.78      0.78      7885\n",
      "\n",
      "==================\" 300 iterations \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.35      0.51      3923\n",
      "           1       0.60      0.99      0.75      3962\n",
      "\n",
      "    accuracy                           0.67      7885\n",
      "   macro avg       0.79      0.67      0.63      7885\n",
      "weighted avg       0.79      0.67      0.63      7885\n",
      "\n",
      "==================\" 500 iterations \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      3923\n",
      "           1       0.88      0.94      0.91      3962\n",
      "\n",
      "    accuracy                           0.90      7885\n",
      "   macro avg       0.91      0.90      0.90      7885\n",
      "weighted avg       0.91      0.90      0.90      7885\n",
      "\n",
      "=================\" 1000 iterations \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      3923\n",
      "           1       0.90      0.92      0.91      3962\n",
      "\n",
      "    accuracy                           0.91      7885\n",
      "   macro avg       0.91      0.91      0.91      7885\n",
      "weighted avg       0.91      0.91      0.91      7885\n",
      "\n",
      "=================\" 3000 iterations \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90      3923\n",
      "           1       0.89      0.92      0.91      3962\n",
      "\n",
      "    accuracy                           0.90      7885\n",
      "   macro avg       0.91      0.90      0.90      7885\n",
      "weighted avg       0.91      0.90      0.90      7885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num in num_of_iterations:\n",
    "    model = LogisticRegression(num_iterations=num)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(('\" '+ str(num) +' iterations \"').center(53, '='))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different thresholds during model training\n",
    "\n",
    "#### In this section we check how different amount of thresholds affects the whole model and how it affects scores, precision etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = [0.3, 0.5, 0.65, 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating models with different threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\" Threshold = 0.3 \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.44      0.60      3923\n",
      "           1       0.64      0.99      0.78      3962\n",
      "\n",
      "    accuracy                           0.71      7885\n",
      "   macro avg       0.81      0.71      0.69      7885\n",
      "weighted avg       0.80      0.71      0.69      7885\n",
      "\n",
      "=================\" Threshold = 0.5 \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.86      3923\n",
      "           1       0.82      0.96      0.88      3962\n",
      "\n",
      "    accuracy                           0.87      7885\n",
      "   macro avg       0.89      0.87      0.87      7885\n",
      "weighted avg       0.89      0.87      0.87      7885\n",
      "\n",
      "=================\" Threshold = 0.65 \"================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.98      0.87      3923\n",
      "           1       0.97      0.74      0.84      3962\n",
      "\n",
      "    accuracy                           0.86      7885\n",
      "   macro avg       0.88      0.86      0.86      7885\n",
      "weighted avg       0.88      0.86      0.86      7885\n",
      "\n",
      "=================\" Threshold = 0.8 \"=================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      3923\n",
      "           1       0.88      0.94      0.91      3962\n",
      "\n",
      "    accuracy                           0.91      7885\n",
      "   macro avg       0.91      0.91      0.91      7885\n",
      "weighted avg       0.91      0.91      0.91      7885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for thresh in threshold:\n",
    "    model = LogisticRegression(threshold=thresh)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(('\" Threshold = '+ str(thresh) +' \"').center(53, '='))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
